ARGUMENT 'distribution_file=SC2246/breseq/07_error_calibration/0.unique_only_coverage_distribution.tab' __ignored__

ARGUMENT 'plot_file=SC2246/breseq/output/calibration/0.unique_coverage.pdf' __ignored__

ARGUMENT 'deletion_propagation_pr_cutoff=0.000289143' __ignored__


R version 3.5.1 (2018-07-02) -- "Feather Spray"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-conda_cos6-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ##
> ##
> ## AUTHORS
> ##
> ## Jeffrey E. Barrick <jeffrey.e.barrick@gmail.com>
> ##
> ## LICENSE AND COPYRIGHT
> ##
> ## Copyright (c) 2008-2010 Michigan State University
> ## Copyright (c) 2011-2017 The University of Texas at Austin
> ##
> ## breseq is free software; you can redistribute it and/or modify it under the
> ## terms the GNU General Public License as published by the Free Software
> ## Foundation; either version 1, or (at your option) any later version.
> ##
> ##
> 
> ## Arguments:
> ##   distribution_file=/path/to/input 
> ##   plot_file=/path/to/output 
> ##   deletion_propagation_pr_cutoff=float
> ##   plot_poisson=0 or 1
> ##   pdf_output=0 or 1
> 
> ## Returns these values printed out to output log
> ## 
> ##  1. print(nb_fit_size); # 0 if fit failed
> ##  2. print(nb_fit_mu);   # 0 if fit failed
> ##  3. print(m)q
> ##  4. print(v)
> ##  5. print(D)
> ##  6. print(deletion_propagation_coverage)
> ##     -1 if it was <1 after fitting (implying reference sequence is deleted)
> ##
> 
> plot_poisson = 0;
> pdf_output = 1;
> 
> this.print.level = 0
> #this.print.level = 2
> 
> for (e in commandArgs()) {
+   ta = strsplit(e,"=",fixed=TRUE)
+   if(! is.na(ta[[1]][2])) {
+     temp = ta[[1]][2]
+  #   temp = as.numeric(temp) #Im only inputting numbers so I added this to recognize scientific notation
+     if(substr(ta[[1]][1],nchar(ta[[1]][1]),nchar(ta[[1]][1])) == "I") {
+       temp = as.integer(temp)
+     }
+     if(substr(ta[[1]][1],nchar(ta[[1]][1]),nchar(ta[[1]][1])) == "N") {
+       temp = as.numeric(temp)
+     }
+     assign(ta[[1]][1],temp)
+     cat("assigned ",ta[[1]][1]," the value of |",temp,"|\n")
+   } else {
+     assign(ta[[1]][1],TRUE)
+     cat("assigned ",ta[[1]][1]," the value of TRUE\n")
+   }
+ }
assigned  /workspace/home/nasirja/covid-19-signal/.snakemake/conda/1f267c8dfdaa0356ebaad13adc66d00a/lib/R/bin/exec/R  the value of TRUE
assigned  --vanilla  the value of TRUE
assigned  distribution_file  the value of | SC2246/breseq/07_error_calibration/0.unique_only_coverage_distribution.tab |
assigned  plot_file  the value of | SC2246/breseq/output/calibration/0.unique_coverage.pdf |
assigned  deletion_propagation_pr_cutoff  the value of | 0.000289143 |
> 
> deletion_propagation_pr_cutoff = as.numeric(deletion_propagation_pr_cutoff);
> 
> ## initialize values to be filled in
> nb_fit_mu = 0
> nb_fit_size = 0
> m = 0
> v = 0
> D = 0
> deletion_propagation_coverage = -1
> 
> min_fraction_included_in_nb_fit = 0.01
> 
> #load data
> X<-read.table(distribution_file, header=T)
> 
> #table might be empty
> if (nrow(X) == 0)
+ {
+   #print out statistics
+   
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> #create the distribution vector and fit
> Y<-rep(X$coverage, X$n)
> m<-mean(Y)
> v<-var(Y)
> D<-v/m
> 
> ###
> ## Smooth the distribution with a moving average window of size 5
> ## so that we can more reliably find it's maximum value
> ###
> 
> ma5 = c(1, 1, 1, 1, 1)/5;
> 
> ## filtering fails if there are too few points
> if (nrow(X) >= 5) {
+   X$ma = filter(X$n, ma5)
+ } else {
+ 	X$ma = X$n
+ }
> 
> i<-0
> max_n <- 0;
> min_i <- max( trunc(m/4), 1 ); #prevents zero for pathological distributions
> max_i <- i;
> for (i in min_i:length(X$ma))
+ {		
+   #cat(i, "\n")
+ 	if (!is.na(X$ma[i]) && (X$ma[i] > max_n))
+ 	{
+ 		max_n = X$ma[i];
+ 		max_i = i;
+ 	}
+ }
> 
> ##
> # Censor data on the right and left of the maximum
> ##
> 
> start_i = max(floor(max_i*0.5), 1);
> end_i = min(ceiling(max_i*1.5), length(X$ma));
> 
> if (start_i == end_i)
+ {
+   print(nb_fit_size);
+   print(nb_fit_mu);
+   
+   print(m)
+   print(v)
+   print(D)
+   
+   print(deletion_propagation_coverage)
+   
+   q()
+ }
> 
> cat("Fitting from coverage of ", start_i, " to ", end_i, ".\n", sep="")
Fitting from coverage of 1 to 5.
> 
> ##
> # Coarse grain so that we are only fitting a number of bins that is 1000-2000
> #
> # The later adjustment for doing the fits this way is to multiply the means
> # of the negative binomial and poisson distributions by the binning number.
> # (The size parameter of the negative binomial doesn't need to be adjusted.)
> ##
> 
> 
> num_per_bin = trunc((end_i - start_i) / 1000)
> 
> if (num_per_bin > 1) 
+ {
+   cat("Coarse-graining for fits\n")
+   start_i_for_fits = trunc(start_i/num_per_bin)
+   end_i_for_fits = ceiling(end_i/num_per_bin)
+   num_bins = end_i - start_i  + 1
+   cat("Fitting from coverage in adjusted bins ", start_i_for_fits, " to ", end_i_for_fits, ".\n", sep="")
+   cat("Number of bins ", num_bins, ". Each bin has ", num_per_bin, " coverage values.\n", sep="")
+ 
+   # Create a new vector where we've added together values in bins
+   X.for.fits = vector("double", end_i_for_fits)
+   for (i in start_i_for_fits:end_i_for_fits)
+   {
+     for (j in 1:num_per_bin)
+     {
+       if (i*num_per_bin+j <= length(X$n))
+       {
+         X.for.fits[i] = X.for.fits[i] + X$n[i*num_per_bin+j]
+       }
+     }
+   }
+ 
+ } else {
+   ## AVOID num_per_bin equalling zero!!
+   X.for.fits = X$n[1:end_i]
+   num_per_bin = 1
+   start_i_for_fits = start_i
+   end_i_for_fits = end_i
+ }
> 
> 
> ##
> # Now perform negative binomial fitting to the censored data
> ##
> 
> inner_total<-0;
> for (i in start_i_for_fits:end_i_for_fits)
+ {
+ 	inner_total = inner_total + X.for.fits[i]; 
+ }
> # Yes: it's correct to use X here because we want the overall total total
> total_total<-sum(X$n);
> 
> ## let's preconstruct these for speed
> dist = vector("double", end_i_for_fits)
> 
> f_nb <- function(par) {
+ 
+ 	mu = par[1];
+ 	size = par[2];
+ 
+   if ((mu <= 0) || (size <= 0))
+   {
+     return(0);
+   }
+   
+   cat(start_i_for_fits, " ", end_i_for_fits, "\n");
+   cat(mu, " ", size, "\n");
+   
+ 	dist<-c()
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+ 		dist[i] <- dnbinom(i, size=size, mu=mu);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (mu, size)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> 
> ## Fit negative binomial 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> nb_fit = NULL
> ## as.numeric prevents overflow in sums involving integers
> mean_estimate = sum((as.numeric(1:end_i_for_fits)*as.numeric(X.for.fits)))/sum(as.numeric(X.for.fits))
> 
> nb_fit_mu = -1
> nb_fit_size = -1
> try_size = 100000
> try_means_index = 1
> #This is a list of different means to test <-  sometimes the actual mean doesn't lead to a fit
> try_means = c(mean_estimate, 
+               end_i_for_fits, 
+               start_i_for_fits, 
+               1*(end_i_for_fits + start_i_for_fits)/4,
+               2*(end_i_for_fits + start_i_for_fits)/4,
+               3*(end_i_for_fits + start_i_for_fits)/4
+               )
>               
>               
> nb_fit = c()
> 
> while ( ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1)) && (try_size > 0.001) && (try_means_index <= length(try_means)))
+ {
+   try_size = try_size / 10
+   try_mean = try_means[try_means_index]
+ 
+   ## SIZE ESTIMATE from the censored data can be negative, so try various values instead
+   cat("Try Mean: ", try_mean, " Size: ", try_size, "\n")
+ 
+   try( suppressWarnings(nb_fit<-nlm(f_nb, c(try_mean, try_size), iterlim=1000, print.level=this.print.level)) )
+ 
+   nb_fit_mu = nb_fit$estimate[1];
+   nb_fit_size = nb_fit$estimate[2];
+ 
+   cat("Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, "\n")
+   
+   if (try_size <= 0.001) {
+     try_size = 100000
+     try_means_index = try_means_index + 1
+   }
+ }
Try Mean:  1.943545  Size:  10000 
1   5 
1.943545   10000 
1   5 
1.943545   10000 
1   5 
1.943547   10000 
1   5 
1.943545   10000.01 
1   5 
1.892413   10000 
1   5 
1.892415   10000 
1   5 
1.892413   10000.01 
1   5 
1.498732   10000 
1   5 
1.498733   10000 
1   5 
1.498732   10000.01 
1   5 
1.613534   10000 
1   5 
1.613536   10000 
1   5 
1.613534   10000.01 
1   5 
1.593757   10000 
1   5 
1.593758   10000 
1   5 
1.593757   10000.01 
1   5 
1.592363   10000 
1   5 
1.592365   10000 
1   5 
1.592363   10000.01 
1   5 
1.592384   10000 
1   5 
1.592386   10000 
1   5 
1.592384   10000.01 
1   5 
1.592384   10000 
1   5 
1.592543   10000 
1   5 
1.592225   10000 
1   5 
1.592384   10001 
1   5 
1.592384   9999 
1   5 
1.592385   10000 
1   5 
1.592544   10000 
1   5 
1.592226   10000 
1   5 
1.592385   10001 
1   5 
1.592385   9999 
Fit Mean:  1.592385  Size:  10000  Code:  2 
Try Mean:  1.943545  Size:  1000 
1   5 
1.943545   1000 
1   5 
1.943545   1000 
1   5 
1.943547   1000 
1   5 
1.943545   1000.001 
1   5 
1.892526   1000 
1   5 
1.892528   1000 
1   5 
1.892526   1000.001 
1   5 
1.498522   1000 
1   5 
1.498523   1000 
1   5 
1.498522   1000.001 
1   5 
1.613621   1000 
1   5 
1.613622   1000 
1   5 
1.613621   1000.001 
1   5 
1.593765   1000 
1   5 
1.593767   1000 
1   5 
1.593765   1000.001 
1   5 
1.592361   1000 
1   5 
1.592363   1000 
1   5 
1.592361   1000.001 
1   5 
1.592382   1000 
1   5 
1.592384   1000 
1   5 
1.592382   1000.001 
1   5 
1.592382   1000 
1   5 
1.592384   1000 
1   5 
1.592382   1000.001 
Fit Mean:  1.592382  Size:  1000  Code:  2 
Try Mean:  1.943545  Size:  100 
1   5 
1.943545   100 
1   5 
1.943545   100 
1   5 
1.943547   100 
1   5 
1.943545   100.0001 
1   5 
1.893637   100 
1   5 
1.893638   100 
1   5 
1.893637   100.0001 
1   5 
1.496395   100 
1   5 
1.496396   100 
1   5 
1.496395   100.0001 
1   5 
1.614477   100 
1   5 
1.614479   100 
1   5 
1.614477   100.0001 
1   5 
1.593837   100.0001 
1   5 
1.593839   100.0001 
1   5 
1.593837   100.0002 
1   5 
1.592317   100.0001 
1   5 
1.592319   100.0001 
1   5 
1.592317   100.0002 
1   5 
1.592341   100.0001 
1   5 
1.592343   100.0001 
1   5 
1.592341   100.0002 
1   5 
1.592352   100.0001 
1   5 
1.592353   100.0001 
1   5 
1.592352   100.0002 
1   5 
1.592386   100.0004 
1   5 
1.592388   100.0004 
1   5 
1.592386   100.0005 
1   5 
1.592432   100.001 
1   5 
1.592434   100.001 
1   5 
1.592432   100.0011 
1   5 
1.592512   100.0029 
1   5 
1.592514   100.0029 
1   5 
1.592512   100.003 
1   5 
1.592638   100.008 
1   5 
1.59264   100.008 
1   5 
1.592638   100.0081 
1   5 
1.592844   100.0217 
1   5 
1.592846   100.0217 
1   5 
1.592844   100.0218 
1   5 
1.593176   100.0579 
1   5 
1.593177   100.0579 
1   5 
1.593176   100.058 
1   5 
1.593712   100.1539 
1   5 
1.593713   100.1539 
1   5 
1.593712   100.154 
1   5 
1.594572   100.4053 
1   5 
1.594574   100.4053 
1   5 
1.594572   100.4054 
1   5 
1.595939   101.0594 
1   5 
1.595941   101.0594 
1   5 
1.595939   101.0595 
1   5 
1.598045   102.7322 
1   5 
1.598046   102.7322 
1   5 
1.598045   102.7323 
1   5 
1.601051   106.8592 
1   5 
1.601053   106.8592 
1   5 
1.601051   106.8594 
1   5 
1.604639   116.3694 
1   5 
1.604641   116.3694 
1   5 
1.604639   116.3695 
1   5 
1.60734   136.3856 
1   5 
1.607342   136.3856 
1   5 
1.60734   136.3857 
1   5 
1.606277   174.9437 
1   5 
1.606279   174.9437 
1   5 
1.606277   174.9439 
1   5 
1.599462   237.5832 
1   5 
1.599463   237.5832 
1   5 
1.599462   237.5835 
1   5 
1.591874   310.3991 
1   5 
1.591876   310.3991 
1   5 
1.591874   310.3994 
1   5 
1.588071   377.4826 
1   5 
1.588073   377.4826 
1   5 
1.588071   377.483 
1   5 
1.586612   464.2801 
1   5 
1.586613   464.2801 
1   5 
1.586612   464.2805 
1   5 
1.587505   609.001 
1   5 
1.587506   609.001 
1   5 
1.587505   609.0016 
1   5 
1.590491   816.8987 
1   5 
1.590493   816.8987 
1   5 
1.590491   816.8995 
1   5 
1.593203   1060.225 
1   5 
1.593205   1060.225 
1   5 
1.593203   1060.226 
1   5 
1.594566   1336.447 
1   5 
1.594567   1336.447 
1   5 
1.594566   1336.449 
1   5 
1.594771   1724.955 
1   5 
1.594772   1724.955 
1   5 
1.594771   1724.956 
1   5 
1.593859   2297.4 
1   5 
1.593861   2297.4 
1   5 
1.593859   2297.402 
1   5 
1.592531   3043.904 
1   5 
1.592532   3043.904 
1   5 
1.592531   3043.907 
1   5 
1.591618   3931.82 
1   5 
1.591619   3931.82 
1   5 
1.591618   3931.823 
1   5 
1.591293   5069.274 
1   5 
1.591295   5069.274 
1   5 
1.591293   5069.279 
1   5 
1.59152   6697.69 
1   5 
1.591521   6697.69 
1   5 
1.59152   6697.697 
1   5 
1.59209   8914.235 
1   5 
1.592092   8914.235 
1   5 
1.59209   8914.244 
1   5 
1.592605   11685.54 
1   5 
1.592607   11685.54 
1   5 
1.592605   11685.55 
1   5 
1.592864   15182.85 
1   5 
1.592865   15182.85 
1   5 
1.592864   15182.86 
1   5 
1.59285   20009.21 
1   5 
1.592852   20009.21 
1   5 
1.59285   20009.23 
1   5 
1.592626   26326.95 
1   5 
1.592628   26326.95 
1   5 
1.592626   26326.98 
1   5 
1.592332   35923.8 
1   5 
1.592334   35923.8 
1   5 
1.592332   35923.83 
1   5 
1.592178   46599.93 
1   5 
1.59218   46599.93 
1   5 
1.592178   46599.98 
1   5 
1.592184   57485.44 
1   5 
1.592185   57485.44 
1   5 
1.592184   57485.49 
1   5 
1.592284   85187.59 
1   5 
1.592286   85187.59 
1   5 
1.592284   85187.67 
1   5 
1.592376   108171.1 
1   5 
1.592377   108171.1 
1   5 
1.592376   108171.2 
1   5 
1.592426   130128.4 
1   5 
1.592428   130128.4 
1   5 
1.592426   130128.5 
1   5 
1.592474   161586.6 
1   5 
1.592475   161586.6 
1   5 
1.592474   161586.7 
1   5 
1.592515   261605.5 
1   5 
1.592516   261605.5 
1   5 
1.592515   261605.7 
1   5 
1.592473   344405.6 
1   5 
1.592475   344405.6 
1   5 
1.592473   344405.9 
1   5 
1.592442   305132.4 
1   5 
1.592461   329086.3 
1   5 
1.592469   339302.1 
1   5 
1.592472   342644.9 
1   5 
1.592473   343800.1 
1   5 
1.592473   344196.9 
1   5 
1.592473   344333.7 
1   5 
1.592473   344380.9 
1   5 
1.592473   344397 
1   5 
1.592473   344402.7 
1   5 
1.592473   344404.7 
1   5 
1.592473   344405.3 
1   5 
1.592475   344405.3 
1   5 
1.592473   344405.7 
Fit Mean:  1.592473  Size:  344405.3  Code:  2 
Try Mean:  1.943545  Size:  10 
1   5 
1.943545   10 
1   5 
1.943545   10 
1   5 
1.943547   10 
1   5 
1.943545   10.00001 
1   5 
1.902813   10.00008 
1   5 
1.902815   10.00008 
1   5 
1.902813   10.00009 
1   5 
1.472916   10.00618 
1   5 
1.472918   10.00618 
1   5 
1.472916   10.00619 
1   5 
1.622384   10.006 
1   5 
1.622385   10.006 
1   5 
1.622384   10.00601 
1   5 
1.593345   10.00858 
1   5 
1.593346   10.00858 
1   5 
1.593345   10.00859 
1   5 
1.590115   10.01148 
1   5 
1.590116   10.01148 
1   5 
1.590115   10.01149 
1   5 
1.58901   10.01608 
1   5 
1.589012   10.01608 
1   5 
1.58901   10.01609 
1   5 
1.584296   10.05488 
1   5 
1.584298   10.05488 
1   5 
1.584296   10.05489 
1   5 
1.578702   10.14368 
1   5 
1.578704   10.14368 
1   5 
1.578702   10.14369 
1   5 
1.569398   10.41202 
1   5 
1.5694   10.41202 
1   5 
1.569398   10.41203 
1   5 
1.557867   11.0553 
1   5 
1.557868   11.0553 
1   5 
1.557867   11.05531 
1   5 
1.546853   12.53849 
1   5 
1.546855   12.53849 
1   5 
1.546853   12.53851 
1   5 
1.545085   15.61171 
1   5 
1.545086   15.61171 
1   5 
1.545085   15.61172 
1   5 
1.563131   21.20265 
1   5 
1.563133   21.20265 
1   5 
1.563131   21.20267 
1   5 
1.589718   28.3699 
1   5 
1.58972   28.3699 
1   5 
1.589718   28.36993 
1   5 
1.604502   35.08879 
1   5 
1.604504   35.08879 
1   5 
1.604502   35.08882 
1   5 
1.610352   43.05028 
1   5 
1.610353   43.05028 
1   5 
1.610352   43.05032 
1   5 
1.608581   56.05687 
1   5 
1.608583   56.05687 
1   5 
1.608581   56.05692 
1   5 
1.599281   75.18551 
1   5 
1.599283   75.18551 
1   5 
1.599281   75.18559 
1   5 
1.590139   98.08684 
1   5 
1.59014   98.08684 
1   5 
1.590139   98.08694 
1   5 
1.585641   123.9642 
1   5 
1.585643   123.9642 
1   5 
1.585641   123.9643 
1   5 
1.585135   160.4308 
1   5 
1.585137   160.4308 
1   5 
1.585135   160.4309 
1   5 
1.588244   213.7438 
1   5 
1.588245   213.7438 
1   5 
1.588244   213.744 
1   5 
1.592324   282.5777 
1   5 
1.592325   282.5777 
1   5 
1.592324   282.578 
1   5 
1.594889   365.7068 
1   5 
1.594891   365.7068 
1   5 
1.594889   365.7072 
1   5 
1.59557   475.6256 
1   5 
1.595572   475.6256 
1   5 
1.59557   475.6261 
1   5 
1.594574   630.4906 
1   5 
1.594576   630.4906 
1   5 
1.594574   630.4913 
1   5 
1.592818   836.7744 
1   5 
1.592819   836.7744 
1   5 
1.592818   836.7752 
1   5 
1.591472   1095.625 
1   5 
1.591474   1095.625 
1   5 
1.591472   1095.626 
1   5 
1.59097   1431.757 
1   5 
1.590971   1431.757 
1   5 
1.59097   1431.758 
1   5 
1.59127   1893.313 
1   5 
1.591271   1893.313 
1   5 
1.59127   1893.315 
1   5 
1.592018   2514.045 
1   5 
1.59202   2514.045 
1   5 
1.592018   2514.047 
1   5 
1.592691   3311.81 
1   5 
1.592692   3311.81 
1   5 
1.592691   3311.813 
1   5 
1.593007   4346.27 
1   5 
1.593009   4346.27 
1   5 
1.593007   4346.275 
1   5 
1.592941   5740.911 
1   5 
1.592943   5740.911 
1   5 
1.592941   5740.917 
1   5 
1.592628   7617.372 
1   5 
1.59263   7617.372 
1   5 
1.592628   7617.38 
1   5 
1.592298   10077.33 
1   5 
1.5923   10077.33 
1   5 
1.592298   10077.34 
1   5 
1.592111   13292.14 
1   5 
1.592113   13292.14 
1   5 
1.592111   13292.15 
1   5 
1.59211   17479.44 
1   5 
1.592112   17479.44 
1   5 
1.59211   17479.46 
1   5 
1.59224   23222.12 
1   5 
1.592241   23222.12 
1   5 
1.59224   23222.14 
1   5 
1.592397   30711.79 
1   5 
1.592398   30711.79 
1   5 
1.592397   30711.82 
1   5 
1.592503   40898.91 
1   5 
1.592505   40898.91 
1   5 
1.592503   40898.95 
1   5 
1.59251   51086.03 
1   5 
1.592512   51086.03 
1   5 
1.59251   51086.08 
1   5 
1.592492   61273.15 
1   5 
1.592494   61273.15 
1   5 
1.592492   61273.21 
1   5 
1.592453   71460.26 
1   5 
1.592454   71460.26 
1   5 
1.592453   71460.34 
1   5 
1.592415   81647.38 
1   5 
1.592417   81647.38 
1   5 
1.592415   81647.46 
Fit Mean:  1.592415  Size:  81647.38  Code:  5 
Try Mean:  1.943545  Size:  1 
1   5 
1.943545   1 
1   5 
1.943545   1 
1   5 
1.943547   1 
1   5 
1.943545   1.000001 
1   5 
1.932136   1.006418 
1   5 
1.932138   1.006418 
1   5 
1.932136   1.006419 
Fit Mean:  -13.08175  Size:  18.40819  Code:  1 
Try Mean:  1.943545  Size:  0.1 
1   5 
1.943545   0.1 
1   5 
1.943545   0.1 
1   5 
1.943547   0.1 
1   5 
1.943545   0.100001 
1   5 
1.942918   0.1248601 
1   5 
1.94292   0.1248601 
1   5 
1.942918   0.1248611 
1   5 
1.821659   0.6445259 
1   5 
1.821661   0.6445259 
1   5 
1.821659   0.6445269 
1   5 
1.409531   1.365957 
1   5 
1.409532   1.365957 
1   5 
1.409531   1.365958 
1   5 
1.514409   1.534486 
1   5 
1.51441   1.534486 
1   5 
1.514409   1.534487 
1   5 
1.649515   2.591709 
1   5 
1.649517   2.591709 
1   5 
1.649515   2.591712 
1   5 
1.621332   3.372747 
1   5 
1.621333   3.372747 
1   5 
1.621332   3.37275 
1   5 
1.568691   4.872717 
1   5 
1.568693   4.872717 
1   5 
1.568691   4.872722 
1   5 
1.590922   6.544401 
1   5 
1.590924   6.544401 
1   5 
1.590922   6.544408 
1   5 
1.589693   8.895572 
1   5 
1.589694   8.895572 
1   5 
1.589693   8.895581 
1   5 
1.592439   11.85897 
1   5 
1.59244   11.85897 
1   5 
1.592439   11.85898 
1   5 
1.591061   15.81076 
1   5 
1.591063   15.81076 
1   5 
1.591061   15.81077 
1   5 
1.593072   20.98736 
1   5 
1.593074   20.98736 
1   5 
1.593072   20.98739 
1   5 
1.591082   27.83632 
1   5 
1.591084   27.83632 
1   5 
1.591082   27.83634 
1   5 
1.593746   36.89341 
1   5 
1.593748   36.89341 
1   5 
1.593746   36.89345 
1   5 
1.590469   48.90765 
1   5 
1.59047   48.90765 
1   5 
1.590469   48.9077 
1   5 
1.594623   64.91064 
1   5 
1.594625   64.91064 
1   5 
1.594623   64.9107 
1   5 
1.589988   86.27738 
1   5 
1.589989   86.27738 
1   5 
1.589988   86.27747 
1   5 
1.594206   114.7136 
1   5 
1.594208   114.7136 
1   5 
1.594206   114.7137 
1   5 
1.591418   152.199 
1   5 
1.591419   152.199 
1   5 
1.591418   152.1992 
1   5 
1.592777   201.6643 
1   5 
1.592779   201.6643 
1   5 
1.592777   201.6645 
1   5 
1.592211   267.1105 
1   5 
1.592212   267.1105 
1   5 
1.592211   267.1108 
1   5 
1.59245   353.7738 
1   5 
1.592451   353.7738 
1   5 
1.59245   353.7741 
1   5 
1.592351   468.5679 
1   5 
1.592353   468.5679 
1   5 
1.592351   468.5684 
1   5 
1.592395   620.641 
1   5 
1.592396   620.641 
1   5 
1.592395   620.6416 
1   5 
1.592377   822.0803 
1   5 
1.592378   822.0803 
1   5 
1.592377   822.0811 
1   5 
1.592385   1088.984 
1   5 
1.592387   1088.984 
1   5 
1.592385   1088.985 
1   5 
1.592382   1442.409 
1   5 
1.592384   1442.409 
1   5 
1.592382   1442.411 
1   5 
1.592384   1910.873 
1   5 
1.592386   1910.873 
1   5 
1.592384   1910.874 
1   5 
1.592383   2530.828 
1   5 
1.592385   2530.828 
1   5 
1.592383   2530.831 
1   5 
1.592384   3353.031 
1   5 
1.592386   3353.031 
1   5 
1.592384   3353.034 
1   5 
1.592384   4441.041 
1   5 
1.592385   4441.041 
1   5 
1.592384   4441.045 
1   5 
1.592384   5883.781 
1   5 
1.592386   5883.781 
1   5 
1.592384   5883.787 
1   5 
1.592384   7799.404 
1   5 
1.592386   7799.404 
1   5 
1.592384   7799.412 
1   5 
1.592384   9745.52 
1   5 
1.592386   9745.52 
1   5 
1.592384   9745.529 
1   5 
1.592384   11691.64 
1   5 
1.592386   11691.64 
1   5 
1.592384   11691.65 
1   5 
1.592384   13637.75 
1   5 
1.592386   13637.75 
1   5 
1.592384   13637.76 
1   5 
1.592384   15583.87 
1   5 
1.592386   15583.87 
1   5 
1.592384   15583.88 
1   5 
1.592384   17529.98 
1   5 
1.592386   17529.98 
1   5 
1.592384   17530 
Fit Mean:  1.592384  Size:  17529.98  Code:  5 
Try Mean:  1.943545  Size:  0.01 
1   5 
1.943545   0.01 
1   5 
1.943545   0.01 
1   5 
1.943547   0.01 
1   5 
1.943545   0.010001 
1   5 
1.943502   0.03994644 
1   5 
1.943504   0.03994644 
1   5 
1.943502   0.03994744 
1   5 
1.900627   0.5001796 
1   5 
1.900629   0.5001796 
1   5 
1.900627   0.5001806 
1   5 
1.704486   0.9726454 
1   5 
1.704487   0.9726454 
1   5 
1.704486   0.9726464 
1   5 
0.988026   2.351334 
1   5 
1.532495   1.303608 
1   5 
1.532496   1.303608 
1   5 
1.532495   1.30361 
1   5 
1.424214   1.778005 
1   5 
1.424216   1.778005 
1   5 
1.424214   1.778007 
1   5 
1.490825   2.232765 
1   5 
1.490827   2.232765 
1   5 
1.490825   2.232767 
1   5 
1.632959   3.679865 
1   5 
1.63296   3.679865 
1   5 
1.632959   3.679869 
1   5 
1.624158   4.583713 
1   5 
1.624159   4.583713 
1   5 
1.624158   4.583717 
1   5 
1.581126   6.540526 
1   5 
1.581128   6.540526 
1   5 
1.581126   6.540533 
1   5 
1.586035   8.617554 
1   5 
1.586037   8.617554 
1   5 
1.586035   8.617563 
1   5 
1.592503   11.64146 
1   5 
1.592505   11.64146 
1   5 
1.592503   11.64147 
1   5 
1.591326   15.45539 
1   5 
1.591327   15.45539 
1   5 
1.591326   15.45541 
1   5 
1.59253   20.5512 
1   5 
1.592531   20.5512 
1   5 
1.59253   20.55122 
1   5 
1.591918   27.23725 
1   5 
1.591919   27.23725 
1   5 
1.591918   27.23728 
1   5 
1.592575   36.09056 
1   5 
1.592577   36.09056 
1   5 
1.592575   36.0906 
1   5 
1.592067   47.78942 
1   5 
1.592069   47.78942 
1   5 
1.592067   47.78947 
1   5 
1.592607   63.27762 
1   5 
1.592608   63.27762 
1   5 
1.592607   63.27768 
1   5 
1.592098   83.78036 
1   5 
1.592099   83.78036 
1   5 
1.592098   83.78045 
1   5 
1.592641   110.9367 
1   5 
1.592643   110.9367 
1   5 
1.592641   110.9368 
1   5 
1.592083   146.9083 
1   5 
1.592085   146.9083 
1   5 
1.592083   146.9084 
1   5 
1.592683   194.5722 
1   5 
1.592685   194.5722 
1   5 
1.592683   194.5724 
1   5 
1.592051   257.7365 
1   5 
1.592053   257.7365 
1   5 
1.592051   257.7367 
1   5 
1.592724   341.4716 
1   5 
1.592726   341.4716 
1   5 
1.592724   341.4719 
1   5 
1.592026   452.5024 
1   5 
1.592027   452.5024 
1   5 
1.592026   452.5028 
1   5 
1.592735   599.7782 
1   5 
1.592737   599.7782 
1   5 
1.592735   599.7788 
1   5 
1.592048   795.1383 
1   5 
1.59205   795.1383 
1   5 
1.592048   795.1391 
1   5 
1.592679   1054.265 
1   5 
1.59268   1054.265 
1   5 
1.592679   1054.266 
1   5 
1.59214   1397.808 
1   5 
1.592142   1397.808 
1   5 
1.59214   1397.809 
1   5 
1.592568   1853.001 
1   5 
1.59257   1853.001 
1   5 
1.592568   1853.003 
1   5 
1.592251   2456.047 
1   5 
1.592253   2456.047 
1   5 
1.592251   2456.049 
1   5 
1.592474   3255.1 
1   5 
1.592476   3255.1 
1   5 
1.592474   3255.104 
1   5 
1.592324   4312.472 
1   5 
1.592325   4312.472 
1   5 
1.592324   4312.477 
1   5 
1.592423   5710.909 
1   5 
1.592425   5710.909 
1   5 
1.592423   5710.914 
1   5 
1.592359   7566.966 
1   5 
1.59236   7566.966 
1   5 
1.592359   7566.974 
1   5 
1.592392   9510.537 
1   5 
1.592393   9510.537 
1   5 
1.592392   9510.546 
1   5 
1.592382   11454.11 
1   5 
1.592383   11454.11 
1   5 
1.592382   11454.12 
1   5 
1.592385   13397.68 
1   5 
1.592386   13397.68 
1   5 
1.592385   13397.69 
1   5 
1.592384   15341.25 
1   5 
1.592386   15341.25 
1   5 
1.592384   15341.26 
1   5 
1.592384   17284.82 
1   5 
1.592386   17284.82 
1   5 
1.592384   17284.84 
Fit Mean:  1.592384  Size:  17284.82  Code:  5 
Try Mean:  1.943545  Size:  0.001 
1   5 
1.943545   0.001 
1   5 
1.943545   0.001 
1   5 
1.943547   0.001 
1   5 
1.943545   0.001001 
1   5 
1.943541   0.03153389 
1   5 
1.943543   0.03153389 
1   5 
1.943541   0.03153489 
1   5 
1.906452   0.4869913 
1   5 
1.906454   0.4869913 
1   5 
1.906452   0.4869923 
1   5 
1.725283   0.9431778 
1   5 
1.725284   0.9431778 
1   5 
1.725283   0.9431788 
1   5 
0.9684499   2.402291 
1   5 
1.545286   1.290197 
1   5 
1.545287   1.290197 
1   5 
1.545286   1.290199 
1   5 
1.406635   1.8029 
1   5 
1.406636   1.8029 
1   5 
1.406635   1.802902 
1   5 
1.484998   2.114792 
1   5 
1.485   2.114792 
1   5 
1.484998   2.114794 
1   5 
1.647004   3.493774 
1   5 
1.647006   3.493774 
1   5 
1.647004   3.493778 
1   5 
1.636796   4.209763 
1   5 
1.636797   4.209763 
1   5 
1.636796   4.209767 
1   5 
1.576437   6.152257 
1   5 
1.576439   6.152257 
1   5 
1.576437   6.152263 
1   5 
1.58167   8.000522 
1   5 
1.581671   8.000522 
1   5 
1.58167   8.00053 
1   5 
1.593139   10.89666 
1   5 
1.593141   10.89666 
1   5 
1.593139   10.89667 
1   5 
1.59128   14.45028 
1   5 
1.591281   14.45028 
1   5 
1.59128   14.4503 
1   5 
1.592493   19.23956 
1   5 
1.592495   19.23956 
1   5 
1.592493   19.23957 
1   5 
1.591965   25.49868 
1   5 
1.591967   25.49868 
1   5 
1.591965   25.4987 
1   5 
1.592495   33.79537 
1   5 
1.592496   33.79537 
1   5 
1.592495   33.79541 
1   5 
1.592149   44.75075 
1   5 
1.59215   44.75075 
1   5 
1.592149   44.75079 
1   5 
1.592505   59.25484 
1   5 
1.592507   59.25484 
1   5 
1.592505   59.2549 
1   5 
1.592208   78.45093 
1   5 
1.59221   78.45093 
1   5 
1.592208   78.45101 
1   5 
1.592512   103.8728 
1   5 
1.592514   103.8728 
1   5 
1.592512   103.8729 
1   5 
1.592227   137.5406 
1   5 
1.592228   137.5406 
1   5 
1.592227   137.5408 
1   5 
1.592521   182.1385 
1   5 
1.592523   182.1385 
1   5 
1.592521   182.1387 
1   5 
1.592229   241.217 
1   5 
1.592231   241.217 
1   5 
1.592229   241.2173 
1   5 
1.592531   319.4854 
1   5 
1.592533   319.4854 
1   5 
1.592531   319.4857 
1   5 
1.592225   423.1842 
1   5 
1.592226   423.1842 
1   5 
1.592225   423.1847 
1   5 
1.592541   560.5877 
1   5 
1.592543   560.5877 
1   5 
1.592541   560.5882 
1   5 
1.59222   742.6609 
1   5 
1.592221   742.6609 
1   5 
1.59222   742.6616 
1   5 
1.592546   983.9685 
1   5 
1.592548   983.9685 
1   5 
1.592546   983.9695 
1   5 
1.592221   1303.849 
1   5 
1.592223   1303.849 
1   5 
1.592221   1303.85 
1   5 
1.59254   1727.749 
1   5 
1.592541   1727.749 
1   5 
1.59254   1727.751 
1   5 
1.592237   2290.074 
1   5 
1.592238   2290.074 
1   5 
1.592237   2290.076 
1   5 
1.592516   3035.31 
1   5 
1.592518   3035.31 
1   5 
1.592516   3035.313 
1   5 
1.592269   4022.716 
1   5 
1.592271   4022.716 
1   5 
1.592269   4022.72 
1   5 
1.592479   5329.774 
1   5 
1.59248   5329.774 
1   5 
1.592479   5329.779 
1   5 
1.592309   7067.898 
1   5 
1.59231   7067.898 
1   5 
1.592309   7067.905 
1   5 
1.592421   9011.443 
1   5 
1.592423   9011.443 
1   5 
1.592421   9011.452 
1   5 
1.592373   10954.99 
1   5 
1.592375   10954.99 
1   5 
1.592373   10955 
1   5 
1.592388   12898.53 
1   5 
1.59239   12898.53 
1   5 
1.592388   12898.55 
1   5 
1.592384   14842.08 
1   5 
1.592385   14842.08 
1   5 
1.592384   14842.09 
1   5 
1.592385   16785.62 
1   5 
1.592386   16785.62 
1   5 
1.592385   16785.64 
Fit Mean:  1.592385  Size:  16785.62  Code:  5 
Try Mean:  5  Size:  10000 
1   5 
5   10000 
1   5 
5   10000 
1   5 
5.000005   10000 
1   5 
5   10000.01 
1   5 
4.91208   10000 
1   5 
4.912085   10000 
1   5 
4.91208   10000.01 
1   5 
4.822997   10000 
1   5 
4.823002   10000 
1   5 
4.822997   10000.01 
1   5 
4.732734   10000 
1   5 
4.732739   10000 
1   5 
4.732734   10000.01 
1   5 
4.641277   10000 
1   5 
4.641282   10000 
1   5 
4.641277   10000.01 
1   5 
4.548616   10000 
1   5 
4.548621   10000 
1   5 
4.548616   10000.01 
1   5 
4.454746   10000 
1   5 
4.45475   10000 
1   5 
4.454746   10000.01 
1   5 
4.359666   10000 
1   5 
4.359671   10000 
1   5 
4.359666   10000.01 
1   5 
4.263385   10000 
1   5 
4.263389   10000 
1   5 
4.263385   10000.01 
1   5 
4.165917   10000 
1   5 
4.165921   10000 
1   5 
4.165917   10000.01 
1   5 
4.067289   10000 
1   5 
4.067293   10000 
1   5 
4.067289   10000.01 
1   5 
3.967541   10000 
1   5 
3.967545   10000 
1   5 
3.967541   10000.01 
1   5 
3.866727   10000 
1   5 
3.866731   10000 
1   5 
3.866727   10000.01 
1   5 
3.76492   10000 
1   5 
3.764924   10000 
1   5 
3.76492   10000.01 
1   5 
3.662217   10000 
1   5 
3.662221   10000 
1   5 
3.662217   10000.01 
1   5 
3.558738   10000 
1   5 
3.558742   10000 
1   5 
3.558738   10000.01 
1   5 
3.454636   10000 
1   5 
3.454639   10000 
1   5 
3.454636   10000.01 
1   5 
3.350096   10000 
1   5 
3.350099   10000 
1   5 
3.350096   10000.01 
1   5 
3.245345   10000 
1   5 
3.245348   10000 
1   5 
3.245345   10000.01 
Fit Mean:  -178.894  Size:  10000  Code:  1 
Try Mean:  5  Size:  1000 
1   5 
5   1000 
1   5 
5   1000 
1   5 
5.000005   1000 
1   5 
5   1000.001 
1   5 
4.912433   1000 
1   5 
4.912438   1000 
1   5 
4.912433   1000.001 
1   5 
4.823708   1000 
1   5 
4.823712   1000 
1   5 
4.823708   1000.001 
1   5 
4.733807   1000 
1   5 
4.733812   1000 
1   5 
4.733807   1000.001 
1   5 
4.642718   1000 
1   5 
4.642723   1000 
1   5 
4.642718   1000.001 
1   5 
4.55043   1000 
1   5 
4.550434   1000 
1   5 
4.55043   1000.001 
1   5 
4.456937   1000 
1   5 
4.456941   1000 
1   5 
4.456937   1000.001 
1   5 
4.362239   1000 
1   5 
4.362243   1000 
1   5 
4.362239   1000.001 
1   5 
4.266343   1000 
1   5 
4.266347   1000 
1   5 
4.266343   1000.001 
1   5 
4.169264   1000 
1   5 
4.169268   1000 
1   5 
4.169264   1000.001 
1   5 
4.071027   1000 
1   5 
4.071031   1000 
1   5 
4.071027   1000.001 
1   5 
3.971672   1000 
1   5 
3.971676   1000 
1   5 
3.971672   1000.001 
1   5 
3.871251   1000 
1   5 
3.871255   1000 
1   5 
3.871251   1000.001 
1   5 
3.769836   1000 
1   5 
3.769839   1000 
1   5 
3.769836   1000.001 
1   5 
3.667519   1000 
1   5 
3.667523   1000 
1   5 
3.667519   1000.001 
1   5 
3.564421   1000 
1   5 
3.564424   1000 
1   5 
3.564421   1000.001 
1   5 
3.460688   1000 
1   5 
3.460692   1000 
1   5 
3.460688   1000.001 
1   5 
3.356505   1000 
1   5 
3.356508   1000 
1   5 
3.356505   1000.001 
1   5 
3.252091   1000 
1   5 
3.252095   1000 
1   5 
3.252091   1000.001 
Fit Mean:  -300.5353  Size:  999.9989  Code:  1 
Try Mean:  5  Size:  100 
1   5 
5   100 
1   5 
5   100 
1   5 
5.000005   100 
1   5 
5   100.0001 
1   5 
4.915829   99.9999 
1   5 
4.915834   99.9999 
1   5 
4.915829   100 
1   5 
4.830551   99.99981 
1   5 
4.830556   99.99981 
1   5 
4.830551   99.99991 
1   5 
4.74415   99.99972 
1   5 
4.744155   99.99972 
1   5 
4.74415   99.99982 
1   5 
4.656611   99.99963 
1   5 
4.656615   99.99963 
1   5 
4.656611   99.99973 
1   5 
4.567922   99.99954 
1   5 
4.567927   99.99954 
1   5 
4.567922   99.99964 
1   5 
4.478078   99.99946 
1   5 
4.478082   99.99946 
1   5 
4.478078   99.99956 
1   5 
4.387074   99.99938 
1   5 
4.387078   99.99938 
1   5 
4.387074   99.99948 
1   5 
4.294913   99.99931 
1   5 
4.294917   99.99931 
1   5 
4.294913   99.99941 
1   5 
4.201605   99.99923 
1   5 
4.20161   99.99923 
1   5 
4.201605   99.99933 
1   5 
4.107169   99.99917 
1   5 
4.107173   99.99917 
1   5 
4.107169   99.99927 
1   5 
4.011633   99.9991 
1   5 
4.011637   99.9991 
1   5 
4.011633   99.9992 
1   5 
3.915037   99.99904 
1   5 
3.915041   99.99904 
1   5 
3.915037   99.99914 
1   5 
3.817438   99.99898 
1   5 
3.817442   99.99898 
1   5 
3.817438   99.99908 
1   5 
3.71891   99.99893 
1   5 
3.718913   99.99893 
1   5 
3.71891   99.99903 
1   5 
3.619545   99.99888 
1   5 
3.619549   99.99888 
1   5 
3.619545   99.99898 
1   5 
3.519464   99.99883 
1   5 
3.519468   99.99883 
1   5 
3.519464   99.99893 
1   5 
3.418812   99.99879 
1   5 
3.418816   99.99879 
1   5 
3.418812   99.99889 
1   5 
3.31777   99.99875 
1   5 
3.317773   99.99875 
1   5 
3.31777   99.99885 
1   5 
3.216551   99.99871 
1   5 
3.216554   99.99871 
1   5 
3.216551   99.99881 
Fit Mean:  -123.9237  Size:  99.9574  Code:  1 
Try Mean:  5  Size:  10 
1   5 
5   10 
1   5 
5   10 
1   5 
5.000005   10 
1   5 
5   10.00001 
1   5 
4.940579   9.993846 
1   5 
4.940584   9.993846 
1   5 
4.940579   9.993856 
1   5 
4.880508   9.98781 
1   5 
4.880513   9.98781 
1   5 
4.880508   9.98782 
1   5 
4.819775   9.981895 
1   5 
4.81978   9.981895 
1   5 
4.819775   9.981905 
1   5 
4.75837   9.976103 
1   5 
4.758375   9.976103 
1   5 
4.75837   9.976113 
1   5 
4.696283   9.970437 
1   5 
4.696288   9.970437 
1   5 
4.696283   9.970447 
1   5 
4.633503   9.964901 
1   5 
4.633507   9.964901 
1   5 
4.633503   9.964911 
1   5 
4.570021   9.959495 
1   5 
4.570026   9.959495 
1   5 
4.570021   9.959505 
1   5 
4.505829   9.954225 
1   5 
4.505833   9.954225 
1   5 
4.505829   9.954235 
1   5 
4.440919   9.949091 
1   5 
4.440923   9.949091 
1   5 
4.440919   9.949101 
1   5 
4.375285   9.944097 
1   5 
4.375289   9.944097 
1   5 
4.375285   9.944107 
1   5 
4.308922   9.939246 
1   5 
4.308927   9.939246 
1   5 
4.308922   9.939256 
1   5 
4.241827   9.934541 
1   5 
4.241831   9.934541 
1   5 
4.241827   9.934551 
1   5 
4.173999   9.929984 
1   5 
4.174003   9.929984 
1   5 
4.173999   9.929994 
1   5 
4.105439   9.925579 
1   5 
4.105443   9.925579 
1   5 
4.105439   9.925589 
1   5 
4.03615   9.921327 
1   5 
4.036154   9.921327 
1   5 
4.03615   9.921337 
1   5 
3.966141   9.917232 
1   5 
3.966145   9.917232 
1   5 
3.966141   9.917242 
1   5 
3.895423   9.913297 
1   5 
3.895427   9.913297 
1   5 
3.895423   9.913306 
1   5 
3.82401   9.909523 
1   5 
3.824014   9.909523 
1   5 
3.82401   9.909533 
1   5 
3.751925   9.905913 
1   5 
3.751929   9.905913 
1   5 
3.751925   9.905923 
1   5 
3.679194   9.902469 
1   5 
3.679198   9.902469 
1   5 
3.679194   9.902479 
1   5 
3.605851   9.899194 
1   5 
3.605854   9.899194 
1   5 
3.605851   9.899204 
1   5 
3.531936   9.896088 
1   5 
3.53194   9.896088 
1   5 
3.531936   9.896098 
1   5 
3.457502   9.893154 
1   5 
3.457505   9.893154 
1   5 
3.457502   9.893164 
1   5 
3.382608   9.890393 
1   5 
3.382611   9.890393 
1   5 
3.382608   9.890403 
1   5 
3.307326   9.887804 
1   5 
3.30733   9.887804 
1   5 
3.307326   9.887814 
1   5 
3.231742   9.885389 
1   5 
3.231745   9.885389 
1   5 
3.231742   9.885399 
1   5 
3.155954   9.883148 
1   5 
3.155957   9.883148 
1   5 
3.155954   9.883157 
1   5 
3.080075   9.881078 
1   5 
3.080078   9.881078 
1   5 
3.080075   9.881088 
Fit Mean:  -124.0007  Size:  6.696782  Code:  1 
Try Mean:  5  Size:  1 
1   5 
5   1 
1   5 
5   1 
1   5 
5.000005   1 
1   5 
5   1.000001 
1   5 
4.99103   0.958434 
1   5 
4.991035   0.958434 
1   5 
4.99103   0.958435 
Fit Mean:  7.971621  Size:  -10.70017  Code:  1 
Try Mean:  5  Size:  0.1 
1   5 
5   0.1 
1   5 
5   0.1 
1   5 
5.000005   0.1 
1   5 
5   0.100001 
1   5 
4.999787   0.1063296 
1   5 
4.999792   0.1063296 
1   5 
4.999787   0.1063306 
1   5 
4.996546   0.1385652 
1   5 
4.996551   0.1385652 
1   5 
4.996546   0.1385662 
1   5 
4.994012   0.1407888 
1   5 
4.994017   0.1407888 
1   5 
4.994012   0.1407898 
1   5 
4.985319   0.1439986 
1   5 
4.985324   0.1439986 
1   5 
4.985319   0.1439996 
1   5 
4.957323   0.1497477 
1   5 
4.957328   0.1497477 
1   5 
4.957323   0.1497487 
1   5 
4.869235   0.1615658 
1   5 
4.86924   0.1615658 
1   5 
4.869235   0.1615668 
1   5 
4.535696   0.1959719 
1   5 
4.535701   0.1959719 
1   5 
4.535696   0.1959729 
1   5 
4.182204   0.2292016 
1   5 
4.182208   0.2292016 
1   5 
4.182204   0.2292026 
1   5 
3.453633   0.303865 
1   5 
3.453637   0.303865 
1   5 
3.453633   0.303866 
1   5 
0.8112592   0.5988163 
1   5 
2.897098   0.3659875 
1   5 
2.8971   0.3659875 
1   5 
2.897098   0.3659885 
Fit Mean:  -2.364417  Size:  0.965824  Code:  1 
Try Mean:  5  Size:  0.01 
1   5 
5   0.01 
1   5 
5   0.01 
1   5 
5.000005   0.01 
1   5 
5   0.010001 
1   5 
4.999992   0.03359626 
1   5 
4.999997   0.03359626 
1   5 
4.999992   0.03359726 
1   5 
4.999243   0.1213211 
1   5 
4.999248   0.1213211 
1   5 
4.999243   0.1213221 
1   5 
4.997488   0.1375715 
1   5 
4.997493   0.1375715 
1   5 
4.997488   0.1375725 
1   5 
4.995338   0.1407943 
1   5 
4.995343   0.1407943 
1   5 
4.995338   0.1407953 
1   5 
4.991615   0.1424934 
1   5 
4.99162   0.1424934 
1   5 
4.991615   0.1424944 
1   5 
4.971202   0.1475914 
1   5 
4.971207   0.1475914 
1   5 
4.971202   0.1475924 
1   5 
4.91711   0.1559356 
1   5 
4.917115   0.1559356 
1   5 
4.91711   0.1559366 
1   5 
4.726412   0.17722 
1   5 
4.726416   0.17722 
1   5 
4.726412   0.177221 
1   5 
3.07126   0.3408986 
1   5 
3.071263   0.3408986 
1   5 
3.07126   0.3408996 
Fit Mean:  -16.82262  Size:  2.355532  Code:  1 
Try Mean:  5  Size:  0.001 
1   5 
5   0.001 
1   5 
5   0.001 
1   5 
5.000005   0.001 
1   5 
5   0.001001 
1   5 
4.999999   0.02660249 
1   5 
5.000004   0.02660249 
1   5 
4.999999   0.02660349 
1   5 
4.999425   0.1188224 
1   5 
4.99943   0.1188224 
1   5 
4.999425   0.1188234 
1   5 
4.997768   0.1369699 
1   5 
4.997773   0.1369699 
1   5 
4.997768   0.1369709 
1   5 
4.995675   0.1407091 
1   5 
4.99568   0.1407091 
1   5 
4.995675   0.1407101 
1   5 
4.992558   0.1421721 
1   5 
4.992563   0.1421721 
1   5 
4.992558   0.1421731 
1   5 
4.972182   0.1473663 
1   5 
4.972187   0.1473663 
1   5 
4.972182   0.1473673 
1   5 
4.919902   0.1555039 
1   5 
4.919907   0.1555039 
1   5 
4.919902   0.1555049 
1   5 
4.732769   0.1764725 
1   5 
4.732774   0.1764725 
1   5 
4.732769   0.1764735 
1   5 
3.213653   0.3263799 
1   5 
3.213657   0.3263799 
1   5 
3.213653   0.3263809 
Fit Mean:  -12.31989  Size:  1.898332  Code:  1 
Try Mean:  1  Size:  10000 
1   5 
1   10000 
1   5 
1   10000 
1   5 
1.000001   10000 
1   5 
1   10000.01 
1   5 
1.169565   10000 
1   5 
1.169566   10000 
1   5 
1.169565   10000.01 
1   5 
1.457728   10000 
1   5 
1.457729   10000 
1   5 
1.457728   10000.01 
1   5 
1.55813   10000 
1   5 
1.558132   10000 
1   5 
1.55813   10000.01 
1   5 
1.58928   10000 
1   5 
1.589281   10000 
1   5 
1.58928   10000.01 
1   5 
1.59231   10000 
1   5 
1.592311   10000 
1   5 
1.59231   10000.01 
1   5 
1.592384   10000 
1   5 
1.592386   10000 
1   5 
1.592384   10000.01 
1   5 
1.592384   10000 
1   5 
1.592386   10000 
1   5 
1.592384   10000.01 
Fit Mean:  1.592384  Size:  10000  Code:  2 
Try Mean:  1  Size:  1000 
1   5 
1   1000 
1   5 
1   1000 
1   5 
1.000001   1000 
1   5 
1   1000.001 
1   5 
1.16938   1000 
1   5 
1.169381   1000 
1   5 
1.16938   1000.001 
1   5 
1.457484   1000 
1   5 
1.457485   1000 
1   5 
1.457484   1000.001 
1   5 
1.558005   1000 
1   5 
1.558006   1000 
1   5 
1.558005   1000.001 
1   5 
1.589256   1000 
1   5 
1.589258   1000 
1   5 
1.589256   1000.001 
1   5 
1.592307   1000 
1   5 
1.592308   1000 
1   5 
1.592307   1000.001 
1   5 
1.592382   1000 
1   5 
1.592384   1000 
1   5 
1.592382   1000.001 
1   5 
1.592382   1000 
1   5 
1.592384   1000 
1   5 
1.592382   1000.001 
Fit Mean:  1.592382  Size:  1000  Code:  2 
Try Mean:  1  Size:  100 
1   5 
1   100 
1   5 
1   100 
1   5 
1.000001   100 
1   5 
1   100.0001 
1   5 
1.167547   100 
1   5 
1.167548   100 
1   5 
1.167547   100.0001 
1   5 
1.455053   100 
1   5 
1.455054   100 
1   5 
1.455053   100.0001 
1   5 
1.556733   100 
1   5 
1.556735   100 
1   5 
1.556733   100.0001 
1   5 
1.588999   100 
1   5 
1.589   100 
1   5 
1.588999   100.0001 
1   5 
1.592256   100 
1   5 
1.592258   100 
1   5 
1.592256   100.0001 
1   5 
1.592341   100 
1   5 
1.592342   100 
1   5 
1.592341   100.0001 
1   5 
1.592342   100 
1   5 
1.592344   100 
1   5 
1.592342   100.0001 
Fit Mean:  1.592342  Size:  100  Code:  2 
Try Mean:  1  Size:  10 
1   5 
1   10 
1   5 
1   10 
1   5 
1.000001   10 
1   5 
1   10.00001 
1   5 
1.150473   9.99967 
1   5 
1.150474   9.99967 
1   5 
1.150473   9.99968 
1   5 
1.431818   10.00008 
1   5 
1.43182   10.00008 
1   5 
1.431818   10.00009 
1   5 
1.542716   10.00106 
1   5 
1.542717   10.00106 
1   5 
1.542716   10.00107 
1   5 
1.584525   10.0021 
1   5 
1.584527   10.0021 
1   5 
1.584525   10.00211 
1   5 
1.59014   10.00289 
1   5 
1.590141   10.00289 
1   5 
1.59014   10.0029 
1   5 
1.590533   10.00361 
1   5 
1.590535   10.00361 
1   5 
1.590533   10.00362 
1   5 
1.593029   10.01305 
1   5 
1.59303   10.01305 
1   5 
1.593029   10.01306 
1   5 
1.595895   10.03435 
1   5 
1.595897   10.03435 
1   5 
1.595895   10.03436 
1   5 
1.60116   10.10318 
1   5 
1.601161   10.10318 
1   5 
1.60116   10.10319 
1   5 
1.608856   10.27842 
1   5 
1.608857   10.27842 
1   5 
1.608856   10.27843 
1   5 
1.619973   10.72669 
1   5 
1.619974   10.72669 
1   5 
1.619973   10.7267 
1   5 
1.63272   11.74727 
1   5 
1.632722   11.74727 
1   5 
1.63272   11.74728 
1   5 
1.641458   13.87739 
1   5 
1.64146   13.87739 
1   5 
1.641458   13.87741 
1   5 
1.635525   17.99068 
1   5 
1.635527   17.99068 
1   5 
1.635525   17.9907 
1   5 
1.60913   24.59867 
1   5 
1.609132   24.59867 
1   5 
1.60913   24.59869 
1   5 
1.585577   31.68289 
1   5 
1.585578   31.68289 
1   5 
1.585577   31.68292 
1   5 
1.576618   38.21587 
1   5 
1.57662   38.21587 
1   5 
1.576618   38.21591 
1   5 
1.575229   48.67103 
1   5 
1.575231   48.67103 
1   5 
1.575229   48.67108 
1   5 
1.582634   65.07732 
1   5 
1.582636   65.07732 
1   5 
1.582634   65.07739 
1   5 
1.592291   86.00578 
1   5 
1.592293   86.00578 
1   5 
1.592291   86.00586 
1   5 
1.597827   109.8887 
1   5 
1.597828   109.8887 
1   5 
1.597827   109.8888 
1   5 
1.599252   141.7703 
1   5 
1.599254   141.7703 
1   5 
1.599252   141.7704 
1   5 
1.596988   188.0457 
1   5 
1.59699   188.0457 
1   5 
1.596988   188.0459 
1   5 
1.59313   249.5686 
1   5 
1.593132   249.5686 
1   5 
1.59313   249.5689 
1   5 
1.590389   325.2405 
1   5 
1.59039   325.2405 
1   5 
1.590389   325.2408 
1   5 
1.589497   423.756 
1   5 
1.589499   423.756 
1   5 
1.589497   423.7565 
1   5 
1.590269   560.9123 
1   5 
1.59027   560.9123 
1   5 
1.590269   560.9129 
1   5 
1.591849   744.583 
1   5 
1.59185   744.583 
1   5 
1.591849   744.5837 
1   5 
1.59312   978.5699 
1   5 
1.593122   978.5699 
1   5 
1.59312   978.5709 
1   5 
1.593614   1283.412 
1   5 
1.593616   1283.412 
1   5 
1.593614   1283.413 
1   5 
1.593361   1697.633 
1   5 
1.593363   1697.633 
1   5 
1.593361   1697.635 
1   5 
1.592705   2252.641 
1   5 
1.592707   2252.641 
1   5 
1.592705   2252.643 
1   5 
1.592117   2972.311 
1   5 
1.592118   2972.311 
1   5 
1.592117   2972.314 
1   5 
1.591853   3913.176 
1   5 
1.591854   3913.176 
1   5 
1.591853   3913.18 
1   5 
1.591929   5174.367 
1   5 
1.59193   5174.367 
1   5 
1.591929   5174.372 
1   5 
1.592205   6858.881 
1   5 
1.592206   6858.881 
1   5 
1.592205   6858.888 
1   5 
1.592476   9066.613 
1   5 
1.592478   9066.613 
1   5 
1.592476   9066.622 
1   5 
1.592613   11949.84 
1   5 
1.592615   11949.84 
1   5 
1.592613   11949.85 
1   5 
1.592597   15833.42 
1   5 
1.592598   15833.42 
1   5 
1.592597   15833.44 
1   5 
1.592482   20786.24 
1   5 
1.592484   20786.24 
1   5 
1.592482   20786.26 
1   5 
1.592348   28083.67 
1   5 
1.59235   28083.67 
1   5 
1.592348   28083.7 
1   5 
1.592279   37271.02 
1   5 
1.592281   37271.02 
1   5 
1.592279   37271.05 
1   5 
1.592281   47320.89 
1   5 
1.592283   47320.89 
1   5 
1.592281   47320.94 
1   5 
1.592325   57370.77 
1   5 
1.592327   57370.77 
1   5 
1.592325   57370.83 
1   5 
1.592361   67420.64 
1   5 
1.592362   67420.64 
1   5 
1.592361   67420.71 
1   5 
1.592387   77470.52 
1   5 
1.592389   77470.52 
1   5 
1.592387   77470.6 
Fit Mean:  1.592387  Size:  77470.52  Code:  1 
> 
> cat("Final Fit Mean: ", nb_fit_mu, " Size: ", nb_fit_size, " Code: ", nb_fit$code, " Try Size: ", try_size, "\n")
Final Fit Mean:  1.592387  Size:  77470.52  Code:  1  Try Size:  10 
> 
> ## Fit failed = reset parameters so graphing and output code can recognize this
> if ((nb_fit_mu < 0) || (nb_fit_size < 0) || (nb_fit$code != 1))
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> 
> ## things can go wrong with fitting and we can still end up with invalid values
> 
> fit_nb = c()
> included_fract = 0
> if (nb_fit_mu > 0)
+ {
+   end_fract = pnbinom(end_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   start_fract = pnbinom(start_i_for_fits, mu = nb_fit_mu, size=nb_fit_size)
+   included_fract = end_fract-start_fract;
+ 
+   if (included_fract >= 0.01) {
+ 
+     ## Adjust so that we are back in full coords before making fit!!
+     if (num_per_bin > 1) 
+     {
+       nb_fit_mu = nb_fit_mu * num_per_bin
+     }
+     fit_nb = dnbinom(0:max(X$coverage), mu = nb_fit_mu, size=nb_fit_size)*inner_total/included_fract;
+   }
+ }
> 
> ## If an insufficient amount of fit was included, then invalidate it
> if (included_fract < 0.01)
+ {
+   nb_fit_mu = 0
+   nb_fit_size = 0
+ }
> 
> f_p <- function(par) {
+ 
+   lambda = par[1];
+ 
+   if (lambda <= 0)
+   {
+     return(0);
+   }
+   
+ 	total <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{	
+     #cat(i, " ", lambda, "\n");
+ 		dist[i] <- dpois(i, lambda=lambda);
+ 		total <- total + dist[i] 
+ 	}
+ 	#print (total)
+ 
+  	l <- 0;
+ 	for (i in start_i_for_fits:end_i_for_fits)
+ 	{
+ 		l <- l + ((X.for.fits[i]/inner_total)-(dist[i]/total))^2;
+ 	}
+ 	return(l);
+ }
> 
> 
> ## Fit Poisson 
> ## - allow fit to fail and set all params to zero/empty if that is the case
> 
> p_fit = NULL
> try(suppressWarnings(p_fit<-nlm(f_p, c(m), print.level=this.print.level)))
> 
> fit_p = c()
> if (!is.null(p_fit) && (p_fit$estimate[1] > 0))
+ {
+   #print (nb_fit$estimate[1])
+   p_fit_lambda = p_fit$estimate[1];
+   #print(0:max(X$coverage))
+ 
+   end_fract = ppois(end_i_for_fits, lambda = p_fit_lambda)
+   start_fract = ppois(start_i_for_fits, lambda = p_fit_lambda)
+   included_fract = end_fract-start_fract;
+ 
+   ## Adjust so that we are back in full coords before making fit!!
+   if (num_per_bin > 1) 
+   {
+     p_fit_lambda = p_fit_lambda * num_per_bin
+   }
+   fit_p<-dpois(0:max(X$coverage), lambda = p_fit_lambda)*inner_total/included_fract;
+ }
> 
> 
> ## Graphing
> ##
> ## don't graph very high values with very little coverage
> i<-max_i
> while (i <= length(X$n) && X$n[i]>0.01*max_n)
+ {		
+ 	i <- i+1;
+ }
> graph_end_i <-i
> 
> ## Ths leaves enough room to the right of the peak for the legend
> graph_end_i = max(floor(2.2 * max_i), graph_end_i);
> 
> ## graphics settings
> my_pch = 21
> my_col = "black";
> my_col_censored = "red";
> 
> if (pdf_output == 0) {
+   
+   ## bitmap() requires ghostscript to be installed.
+   ## taa=4, gaa=2 options NOT compatible with earlier R versions!
+   ## units = "px" NOT compatible with even earlier R versions!
+   
+   if(!capabilities(what = "png"))
+   {
+     ## fallback to ghostscript
+     bitmap(plot_file, height=6, width=7, type = "png16m", res = 72, pointsize=18)
+   } else {
+     ## use X11 function, which gives better resolution
+     png(plot_file, height=6, width=7, units ="in", res = 72, pointsize=18)
+     par(family="sans")
+   }
+ } else {
+   pdf(plot_file, height=6, width=7)
+   par(family="sans")
+ }
> 
> par(mar=c(5.5,7.5,3,1.5));
> 
> max_y = 0
> if (plot_poisson) {
+ 	max_y = max(X$n, fit_p, fit_nb)
+ } else {
+ 	max_y = max(X$n, fit_nb)
+ }
> 
> plot(0:10, 0:10, type="n", lty="solid", ylim=c(0, max_y)*1.05, xlim=c(0, graph_end_i), lwd=1, xaxs="i", yaxs="i", axes=F, las=1, main="Coverage Distribution at Unique-Only Positions", xlab="Coverage depth (reads)", ylab="", cex.lab=1.2, cex.axis=1.2)
> 
> mtext(side = 2, text = "Number of reference positions", line = 5.5, cex=1.2)
> 
> sciNotation <- function(x, digits = 1) {
+     if (length(x) > 1) {
+         return(append(sciNotation(x[1]), sciNotation(x[-1])))     
+ 	} 
+     if (!x) return(0) 
+ 
+ 	exponent <- floor(log10(x)) 
+     base <- round(x / 10^exponent, digits)     
+ 	as.expression(substitute(base %*% 10^exponent, list(base = base, exponent = exponent))) 
+ }
> 
> #axis(2, cex.lab=1.2, las=1, cex.axis=1.2, labels=T, at=(0:6)*50000)
> axis(2, cex.lab=1.2, las=1, cex.axis=1.2, at = axTicks(2), labels = sciNotation(axTicks(2), 1))
> axis(1, cex.lab=1.2, cex.axis=1.2, labels=T)
> box()
> 
> #graph the coverage as points
> fit_data <- subset(X, (coverage>=start_i) & (coverage<=end_i) );
> points(fit_data$coverage, fit_data$n, pch=my_pch, col=my_col, bg="white", cex=1.2)
> 
> #graph the censored coverage as red points
> cat(start_i, " ", end_i, "\n", sep="")
1 5
> 
> censored_data <- subset(X, (coverage<start_i) | (coverage>end_i) );
> points(censored_data$coverage, censored_data$n, pch=my_pch, col=my_col_censored, bg="white", cex=1.2)
> 
> #graph the poisson fit IF REQUESTED
> if (plot_poisson) {
+ 	lines(0:max(X$coverage), fit_p, lwd=3, lty="22", col="black");
+ }
> 
> #graph the negative binomial fit
> if (nb_fit_mu > 0) {
+   lines(0:max(X$coverage), fit_nb, lwd=3, col="black");
+ }
> 
> if (plot_poisson) {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial", "Poisson"), lty=c("blank","blank","solid","22"), lwd=c(1,1,2,2), pch=c(my_pch, my_pch, -1, -1), col=c("black", "red", "black", "black"), bty="n")
+ } else {
+ 	legend("topright", c("Coverage distribution", "Censored data", "Negative binomial"), lty=c("blank","blank","solid"), lwd=c(1,1,2), pch=c(my_pch, my_pch, -1), col=c("black", "red", "black"), bty="n")
+ }
> 
> dev.off()
null device 
          1 
> 
> ## Fit the marginal value that we use for propagating deletions
> 
> if (nb_fit_mu > 0) {
+   cat(nb_fit_size, " ", nb_fit_mu, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = nb_fit_size, mu = nb_fit_mu))
+ } else {
+   cat("Fallback to calculating off an estimate of just variance = mu + mu^2/size\n")
+   size_estimate = (1/(v-m))*(m*m)
+   cat("Mu estimate=", m," Size estimate =", size_estimate, "\n")
+   deletion_propagation_coverage = suppressWarnings(qnbinom(deletion_propagation_pr_cutoff, size = size_estimate, mu = m))
+   if (is.na(deletion_propagation_coverage) || is.nan(deletion_propagation_coverage) || (deletion_propagation_coverage < 1)) {
+     cat("Double fallback to calculating as just 10% of the mean\n")
+     deletion_propagation_coverage = m * 0.1
+   }
+ }
77470.52   1.592387 
> 
> #Don't allow one read to indicate non-deleted regions
> if (deletion_propagation_coverage < 1) {
+     deletion_propagation_coverage = 1
+ }
> 
> #This works fine with the negative values
> #If we have both low fit coverage and low straight average coverage then we're deleted...
> if ( (nb_fit_mu <= 3) && (m <= 3) ) {
+   deletion_propagation_coverage = -1
+ }
> 
> #print out statistics
> 
> print(nb_fit_size);
[1] 77470.52
> print(nb_fit_mu);
[1] 1.592387
> 
> print(m)
[1] 2.541416
> print(v)
[1] 46.9792
> print(D)
[1] 18.48544
> 
> print(deletion_propagation_coverage)
[1] -1
> 
> warnings()
> 
